{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network used for the paper Suelves. et al (2022), in preparation. The repository includes the training saved weights, but the training+test dataset should be retrieved from Casjobs using the indications of the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For any questions, please do not hesitate to contact me on luis.suelves@ncbj.gov.pl or luiseduardosuelves@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version \n",
    "\n",
    "print(python_version()) # The versions used by L.E. Suelves for the paper was 3.8.5\n",
    "\n",
    "import tensorflow as tf\n",
    "# import tensorflow\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if GPUs. If there are, some code to fix cuDNN bugs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print('No GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_file = \n",
    "\n",
    "USE_COLUMNS = ['skyErr_u', 'skyErr_g','skyErr_r', 'skyErr_i', 'skyErr_z']\n",
    "\n",
    "USE_COLUMNS_short = ['skyE-u', 'skyE-g', 'skyE-r', 'skyE-i', 'skyE-z']\n",
    "\n",
    "\n",
    "CLASS_NAMES = ['merger', 'nonmerger'] # only used for the numbering!!\n",
    "NO_CLASS = len(CLASS_NAMES)\n",
    "print(NO_CLASS)\n",
    "CLASS_COLUMN = 'cl'\n",
    "\n",
    "EPOCHS = 1500\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 10000\n",
    "\n",
    "STEPS_PER_EPOCH = 1\n",
    "STEPS_PER_VALID_EPOCH = 1\n",
    "STEPS_PER_TEST_EPOCH = 1\n",
    "\n",
    "train_photo_count = 0\n",
    "valid_photo_count = 0\n",
    "test_photo_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(file_path, class_column, class_number, use_columns, err_cols=False, multi_cols=None):\n",
    "    ### Convert the data from the dataset .fits file into a table\n",
    "    table = Table.read(file_path)\n",
    "\n",
    "    labels = tf.one_hot(table[class_column].data, class_number)\n",
    "    data = []\n",
    "    for column in use_columns:\n",
    "        data.append(table[column].data)\n",
    "    data = np.array(data)    \n",
    "    data = data.T\n",
    "\n",
    "    return np.hstack((data,labels))\n",
    "\n",
    "pre_error_tab = prepare_data(name_file, CLASS_COLUMN, NO_CLASS, USE_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_iteration(table, set_type):\n",
    "    ### Creates the tensorflow data using the table obtained in prepare_data()\n",
    "    ### It shuffles the batchs randomly, which is useful for training\n",
    "    data = table[:,:5]\n",
    "    labels = table[:,5:]\n",
    "    \n",
    "    if 'Train' in set_type:\n",
    "        global train_photo_count\n",
    "        global STEPS_PER_EPOCH\n",
    "        train_photo_count = len(data)\n",
    "        STEPS_PER_EPOCH = np.ceil(train_photo_count/BATCH_SIZE).astype(int)\n",
    "        batch_size = BATCH_SIZE\n",
    "    elif 'Valid' in set_type:\n",
    "        global valid_photo_count\n",
    "        global STEPS_PER_VALID_EPOCH\n",
    "        valid_photo_count = len(data)\n",
    "        #STEPS_PER_VALID_EPOCH = np.floor(valid_photo_count/BATCH_SIZE).astype(int)\n",
    "        STEPS_PER_VALID_EPOCH = 1\n",
    "        batch_size = valid_photo_count\n",
    "    \n",
    "    data = np.log10(data)\n",
    "#     print(data[0])\n",
    "        \n",
    "    ds = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "#     print(next(iter(ds.as_numpy_iterator())))\n",
    "    ds = ds.shuffle(SHUFFLE_BUFFER_SIZE).batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_iteration(pre_error_tab,set_type='Valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_iteration_noshufle(table, set_type):\n",
    "    ### Creates the tensorflow data using the table obtained in prepare_data()\n",
    "    ### It does not shuffle the batchs randomly, avoiding problems when plotting the dataset\n",
    "    data = table[:,:5]\n",
    "    labels = table[:,5:]\n",
    "    \n",
    "    if 'Train' in set_type:\n",
    "        global train_photo_count\n",
    "        global STEPS_PER_EPOCH\n",
    "        train_photo_count = len(data)\n",
    "        STEPS_PER_EPOCH = np.ceil(train_photo_count/BATCH_SIZE).astype(int)\n",
    "        batch_size = BATCH_SIZE\n",
    "    elif 'Valid' in set_type:\n",
    "        global valid_photo_count\n",
    "        global STEPS_PER_VALID_EPOCH\n",
    "        valid_photo_count = len(data)\n",
    "        #STEPS_PER_VALID_EPOCH = np.floor(valid_photo_count/BATCH_SIZE).astype(int)\n",
    "        STEPS_PER_VALID_EPOCH = 1\n",
    "        batch_size = valid_photo_count\n",
    "    \n",
    "    data = np.log10(data)\n",
    "    print(data[0])\n",
    "    ds = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "    print(next(iter(ds.as_numpy_iterator())))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_iteration_noshufle(pre_error_tab,set_type='Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class photo_model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        ## Im trying to name each layer and see if the loading is managed through it\n",
    "        super(photo_model, self).__init__()\n",
    "        self.drop_rate = 0.1\n",
    "        \n",
    "        self.fuco1 = tf.keras.layers.Dense(16,name='dens_1')\n",
    "        self.batn1 = tf.keras.layers.BatchNormalization(name='btchn_1')\n",
    "        self.drop1 = tf.keras.layers.Dropout(self.drop_rate,name='drop_1')\n",
    "        \n",
    "        self.fuco5 = tf.keras.layers.Dense(16,name='dens_2')\n",
    "        self.batn5 = tf.keras.layers.BatchNormalization(name='btchn_2')\n",
    "        self.drop5 = tf.keras.layers.Dropout(self.drop_rate,name='drop_1')\n",
    "        \n",
    "        self.y_out = tf.keras.layers.Dense(NO_CLASS, activation='softmax',name='out')\n",
    "        \n",
    "    def call(self, x, training=True):\n",
    "        \n",
    "        x = self.fuco1(x)\n",
    "        x = self.batn1(x)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.drop1(x, training=training)\n",
    "        \n",
    "        x = self.fuco5(x)\n",
    "        x = self.batn5(x)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        x = self.drop5(x, training=training)\n",
    "        \n",
    "        return self.y_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = tf.keras.metrics.CategoricalAccuracy(name='val_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(data, labels):\n",
    "    '''labels shoule be one_hot'''\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model(data)\n",
    "        loss = total_loss(labels, pred)\n",
    "        mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "    #Update gradients and optimize\n",
    "    grads = tape.gradient(mean_loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    #tf statistics tracking\n",
    "    train_loss(mean_loss)\n",
    "    train_accuracy(labels, pred)\n",
    "\n",
    "@tf.function\n",
    "def val_step(data, labels):\n",
    "    '''labels should be one_hot'''\n",
    "    pred = model(data, training=False)\n",
    "    v_loss = total_loss(labels, pred)\n",
    "    mean_v_loss = tf.reduce_mean(v_loss)\n",
    "\n",
    "    #tf statistics tracking\n",
    "    val_loss(mean_v_loss)\n",
    "    val_accuracy(labels, pred)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = photo_model()\n",
    "\n",
    "total_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('number of fold-training epochs',EPOCHS)\n",
    "peak = [0, 0, 100]\n",
    "savedpeak = []\n",
    "\n",
    "kt_los = []\n",
    "kt_acc = []\n",
    "kv_los = []\n",
    "kv_acc = []\n",
    "\n",
    "template = 'Epoch {}\\nTrain Loss: {:.3g}, Train Accuracy: {:.3g}\\nValid Loss: {:.3g}, Valid Accuracy: {:.3g}'\n",
    "\n",
    "k = 5\n",
    "skfold = StratifiedKFold(k, False)\n",
    "sk_data = pre_error_tab[:,:5]\n",
    "sk_labels = pre_error_tab[:,5]\n",
    "\n",
    "www = []\n",
    "fold_no = 1\n",
    "for train_sk, valid_sk in skfold.split(sk_data,sk_labels):\n",
    "    print('fold number: {}, train: {}, valid:{}' .format(fold_no,len(train_sk), len(valid_sk)))\n",
    "    train_ds = kfold_iteration(pre_error_tab[train_sk],set_type='Train')\n",
    "    valid_ds = kfold_iteration(pre_error_tab[valid_sk],set_type='Valid')\n",
    "    print(valid_ds)\n",
    "    t_los = []\n",
    "    t_acc = []\n",
    "    v_los = []\n",
    "    v_acc = []\n",
    "    www_temp = []\n",
    "    tf.keras.backend.clear_session()\n",
    "    peak = [0, 0, 100]\n",
    "    \n",
    "    ## Here I create the full model again\n",
    "    ## It also semed necessary to initialize again the train and validation \\\n",
    "    ##    steps that will be used in the following Epoch loop!\n",
    "    \n",
    "    model = photo_model()\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "    val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "    val_accuracy = tf.keras.metrics.CategoricalAccuracy(name='val_accuracy')\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(data, labels):\n",
    "        '''labels shoule be one_hot'''\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = model(data)\n",
    "            loss = total_loss(labels, pred)\n",
    "            mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "        #Update gradients and optimize\n",
    "        grads = tape.gradient(mean_loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "        #tf statistics tracking\n",
    "        train_loss(mean_loss)\n",
    "        train_accuracy(labels, pred)\n",
    "\n",
    "    @tf.function\n",
    "    def val_step(data, labels):\n",
    "        '''labels should be one_hot'''\n",
    "        pred = model(data, training=False)\n",
    "        v_loss = total_loss(labels, pred)\n",
    "        mean_v_loss = tf.reduce_mean(v_loss)\n",
    "\n",
    "        #tf statistics tracking\n",
    "        val_loss(mean_v_loss)\n",
    "        val_accuracy(labels, pred)\n",
    "        return pred\n",
    "    \n",
    "    ## Epoch loop\n",
    "    for epoch in range(0, EPOCHS):\n",
    "    \n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        val_loss.reset_states()\n",
    "        val_accuracy.reset_states()\n",
    "        \n",
    "        #Train\n",
    "        for step in range(0, STEPS_PER_EPOCH):\n",
    "            x_batch, y_batch = next(iter(train_ds))\n",
    "            train_step(x_batch, y_batch)\n",
    "    \n",
    "        #Validate  \n",
    "        y_val_all = None\n",
    "        val_pred = None\n",
    "        for step in range(0, STEPS_PER_VALID_EPOCH):\n",
    "            x_val, y_val = next(iter(valid_ds))\n",
    "            if y_val_all is None:\n",
    "                y_val_all = y_val\n",
    "            else:\n",
    "                y_val_all = np.vstack((y_val_all, y_val))\n",
    "            pred = val_step(x_val, y_val)\n",
    "            if val_pred is None:\n",
    "                val_pred = pred\n",
    "            else:\n",
    "                val_pred = np.vstack((val_pred, pred))\n",
    "    \n",
    "        if epoch%100 == 0:\n",
    "            print(template.format(epoch+1,\n",
    "                                  train_loss.result(), train_accuracy.result(),\n",
    "                                  val_loss.result(), val_accuracy.result()))\n",
    "    \n",
    "        t_los.append(train_loss.result())\n",
    "        t_acc.append(train_accuracy.result())\n",
    "        v_los.append(val_loss.result())\n",
    "        v_acc.append(val_accuracy.result())\n",
    "        \n",
    "        kt_los.append(train_loss.result())\n",
    "        kt_acc.append(train_accuracy.result())\n",
    "        kv_los.append(val_loss.result())\n",
    "        kv_acc.append(val_accuracy.result())\n",
    "    \n",
    "        if val_loss.result() <= peak[2] and val_accuracy.result() >= peak[1]:\n",
    "            peak[0] = epoch+1\n",
    "            peak[1] = val_accuracy.result()\n",
    "            peak[2] = val_loss.result()\n",
    "            savedpeak.append((peak[0],np.round(peak[1],3),np.round(peak[2],3),))\n",
    "            www_temp = model.get_weights()\n",
    "            print('Saved')\n",
    "    \n",
    "    fold_no = fold_no + 1\n",
    "    www.append(www_temp)\n",
    "\n",
    "print('Peaks at Epoch', peak[0], 'with accuracy', np.round(peak[1],3), 'and loss', np.round(peak[2],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Peaks at Epoch', peak[0], 'with accuracy', np.round(peak[1],3), 'and loss', np.round(peak[2],3))\n",
    "fig=plt.figure(figsize=(15,9))\n",
    "plt.plot(kt_los,label='tr ls')\n",
    "plt.plot(kt_acc,label='tr ac')\n",
    "plt.plot(kv_los,label='vl ls')\n",
    "plt.plot(kv_acc,label='vl ac')\n",
    "plt.legend()\n",
    "# plt.ylim(0.55,0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "savedpeak   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNNpeaks(array):\n",
    "    ## Takes de savedpeak list from the NN training, \n",
    "    ## gets the epoch position of the peaks, and extracts the loss and accuracy\n",
    "    \n",
    "    epoch_ini = array[0][0]\n",
    "    epoch_1 = epoch_ini ## necessary so that the comparison works well\n",
    "    peaks = []\n",
    "    for i in array:\n",
    "        if i[0] >= epoch_1: ## necessary so that the first element does not get arbitrarily saved\n",
    "#             print(i[0],epoch_1)\n",
    "            epoch_1 = i[0]  ## necessary so that the loop can work\n",
    "            epoch_2_full = i   ## necessary so that the pleak can be saved in the next iteration,\n",
    "                               ## when the epoch is smaller and gets saved\n",
    "        else:\n",
    "#             print('saved?',i[0],epoch_1)\n",
    "            epoch_1 = i[0]\n",
    "            peaks.append(epoch_2_full)\n",
    "    peaks.append(i) ## Because the last one will never be get through the loop, but would definitely \n",
    "                    ## will be get as the last input of the array\n",
    "    accu = []\n",
    "    loss = []\n",
    "    for i in peaks:\n",
    "        accu.append(i[1])\n",
    "        loss.append(i[2])\n",
    "    \n",
    "    return accu, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNaccu, NNloss = getNNpeaks(savedpeak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(NNaccu), np.mean(NNloss) # Try Jul 15 (0.69979995, 0.65599996) # Try Jul 15 again (0.69940007, 0.6548)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(NNaccu), np.std(NNaccu)/np.sqrt(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./w0_skyErr_pre-norm-log.npy',www[0])\n",
    "np.save('./w1_skyErr_pre-norm-log.npy',www[1])\n",
    "np.save('./w2_skyErr_pre-norm-log.npy',www[2])\n",
    "np.save('./w3_skyErr_pre-norm-log.npy',www[3])\n",
    "np.save('./w4_skyErr_pre-norm-log.npy',www[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to check the performance on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "www0 = np.load('./w0_skyErr_pre-norm-log.npy',allow_pickle=True)\n",
    "www1 = np.load('./w1_skyErr_pre-norm-log.npy',allow_pickle=True)\n",
    "www2 = np.load('./w2_skyErr_pre-norm-log.npy',allow_pickle=True)\n",
    "www3 = np.load('./w3_skyErr_pre-norm-log.npy',allow_pickle=True)\n",
    "www4 = np.load('./w4_skyErr_pre-norm-log.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "www = []\n",
    "www.append(www0)\n",
    "www.append(www1)\n",
    "www.append(www2)\n",
    "www.append(www3)\n",
    "www.append(www4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = '/home/phd11/Photometric_Classification//primary_countE_Inputs/ErrorInputs_fiber_test2.fits'\n",
    "test_tab = prepare_data(test_file, CLASS_COLUMN, NO_CLASS, USE_COLUMNS)\n",
    "test_ds = kfold_iteration(test_tab,set_type='Valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In order to get the accuracy on the test set, we apply the NN on it and calculate the classification types, \\\n",
    "### making used of the 5 save weights, corresponding to the 5 cross-validations folds.\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = photo_model()\n",
    "acc_test = []\n",
    "\n",
    "### An initial run of the model is required as initialization\n",
    "x_batch_ini, y_batch_ini = next(iter(test_ds))\n",
    "buena_ts_ini = model(x_batch_ini, training=False)\n",
    "for i in range(0,5):\n",
    "    model.set_weights(www[i])\n",
    "    for step in range(0, STEPS_PER_TEST_EPOCH):\n",
    "        ### Although I only use 1 Test epoch, might be handy in case one is interested on modifying it\n",
    "        x_batch, y_batch = next(iter(test_ds))\n",
    "        buena_ts = model(x_batch, training=False)\n",
    "\n",
    "    buena_ts = buena_ts.numpy()\n",
    "    ts = buena_ts[:,0]\n",
    "\n",
    "    TPtest = np.where((y_batch.numpy()[:,0]==0) & (buena_ts[:,0]<0.5))[0]\n",
    "    FNtest = np.where((y_batch.numpy()[:,0]==0) & (buena_ts[:,0]>=0.5))[0]\n",
    "    TNtest = np.where((y_batch.numpy()[:,0]==1) & (buena_ts[:,0]>=0.5))[0]\n",
    "\n",
    "    accuracy = (len(TPtest)+ len(TNtest))/len(ts)\n",
    "    acc_test.append(accuracy)\n",
    "    print(accuracy, len(TPtest), len(TNtest),len(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(acc_test),np.std(acc_test)/np.sqrt(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
